

<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>Evaluation of RL models in other processes &mdash; RLStructures  documentation</title>
  

  
  <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="../_static/graphviz.css" type="text/css" />

  
  
  
  

  
  <!--[if lt IE 9]>
    <script src="../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
    
      <script type="text/javascript" id="documentation_options" data-url_root="../" src="../_static/documentation_options.js"></script>
        <script src="../_static/jquery.js"></script>
        <script src="../_static/underscore.js"></script>
        <script src="../_static/doctools.js"></script>
        <script src="../_static/language_data.js"></script>
    
    <script type="text/javascript" src="../_static/js/theme.js"></script>

    
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Implemeting Actor-Critic" href="a2c.html" />
    <link rel="prev" title="Implementing Reinforce with rlstructures" href="reinforce.html" /> 
</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">
    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
          

          
            <a href="../index.html" class="icon icon-home" alt="Documentation Home"> RLStructures
          

          
          </a>

          
            
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        
        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <p class="caption"><span class="caption-text">Getting Started</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../overview.html">Overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="../gettingstarted/index.html">Getting Started</a></li>
<li class="toctree-l1 current"><a class="reference internal" href="index.html">Tutorials</a><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="reinforce.html">Implementing Reinforce with rlstructures</a></li>
<li class="toctree-l2 current"><a class="current reference internal" href="#">Evaluation of RL models in other processes</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#creation-of-the-evaluation-batcher">Creation of the evaluation batcher</a></li>
<li class="toctree-l3"><a class="reference internal" href="#running-the-evaluation-batcher">Running the evaluation batcher</a></li>
<li class="toctree-l3"><a class="reference internal" href="#getting-trajectories-without-blocking-the-learning">Getting trajectories without blocking the learning</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="a2c.html">Implemeting Actor-Critic</a></li>
<li class="toctree-l2"><a class="reference internal" href="recurrent_policy.html">Implemeting Recurrent Policies</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../algorithms/index.html">Provided Algorithms</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api/index.html">RLStructures API</a></li>
<li class="toctree-l1"><a class="reference internal" href="../autoapi/index.html">API Reference</a></li>
</ul>

            
          
        </div>
        
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index.html">RLStructures</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="../index.html" class="icon icon-home"></a> &raquo;</li>
        
          <li><a href="index.html">Tutorials</a> &raquo;</li>
        
      <li>Evaluation of RL models in other processes</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
            
            <a href="../_sources/tutorial/reinforce_with_evaluation.rst.txt" rel="nofollow"> View page source</a>
          
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <div class="section" id="evaluation-of-rl-models-in-other-processes">
<h1>Evaluation of RL models in other processes<a class="headerlink" href="#evaluation-of-rl-models-in-other-processes" title="Permalink to this headline">¶</a></h1>
<p>Regarding the REINFORCE implementation, one missing aspect is a good evaluation of the policy:
* the evaluation has to be done with the <cite>deterministic</cite> policy (while learning is made with the stochastic policy)
* the evaluation over N episodes may be long, and we would like to avoid to slow down the learning</p>
<p>To solve this issue, we will use another batcher in <cite>asynchronous</cite> mode.</p>
<div class="section" id="creation-of-the-evaluation-batcher">
<h2>Creation of the evaluation batcher<a class="headerlink" href="#creation-of-the-evaluation-batcher" title="Permalink to this headline">¶</a></h2>
<p>The evaluation batcher can be created like the trainig batcher (but with a different number of threads and slots)</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">model</span><span class="o">=</span><span class="n">copy</span><span class="o">.</span><span class="n">deepcopy</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">learning_model</span><span class="p">)</span>
<span class="bp">self</span><span class="o">.</span><span class="n">evaluation_batcher</span><span class="o">=</span><span class="n">EpisodeBatcher</span><span class="p">(</span>
    <span class="n">n_timesteps</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="p">[</span><span class="s2">&quot;max_episode_steps&quot;</span><span class="p">],</span>
    <span class="n">n_slots</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="p">[</span><span class="s2">&quot;n_evaluation_episodes&quot;</span><span class="p">],</span>
    <span class="n">create_agent</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_create_agent</span><span class="p">,</span>
    <span class="n">create_env</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_create_env</span><span class="p">,</span>
    <span class="n">env_args</span><span class="o">=</span><span class="p">{</span>
        <span class="s2">&quot;n_envs&quot;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="p">[</span><span class="s2">&quot;n_envs&quot;</span><span class="p">],</span>
        <span class="s2">&quot;max_episode_steps&quot;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="p">[</span><span class="s2">&quot;max_episode_steps&quot;</span><span class="p">],</span>
        <span class="s2">&quot;env_name&quot;</span><span class="p">:</span><span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="p">[</span><span class="s2">&quot;env_name&quot;</span><span class="p">]</span>
    <span class="p">},</span>
    <span class="n">agent_args</span><span class="o">=</span><span class="p">{</span><span class="s2">&quot;n_actions&quot;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_actions</span><span class="p">,</span> <span class="s2">&quot;model&quot;</span><span class="p">:</span> <span class="n">model</span><span class="p">},</span>
    <span class="n">n_threads</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="p">[</span><span class="s2">&quot;n_evaluation_threads&quot;</span><span class="p">],</span>
    <span class="n">seeds</span><span class="o">=</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="p">[</span><span class="s2">&quot;env_seed&quot;</span><span class="p">]</span><span class="o">+</span><span class="n">k</span><span class="o">*</span><span class="mi">10</span> <span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="p">[</span><span class="s2">&quot;n_evaluation_threads&quot;</span><span class="p">])],</span>
<span class="p">)</span>
</pre></div>
</div>
</div>
<div class="section" id="running-the-evaluation-batcher">
<h2>Running the evaluation batcher<a class="headerlink" href="#running-the-evaluation-batcher" title="Permalink to this headline">¶</a></h2>
<p>Running the evaluation batcher is made through <cite>execute</cite>:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">n_episodes</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="p">[</span><span class="s2">&quot;n_evaluation_episodes&quot;</span><span class="p">]</span>
<span class="n">agent_info</span><span class="o">=</span><span class="n">DictTensor</span><span class="p">({</span><span class="s2">&quot;stochastic&quot;</span><span class="p">:</span><span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="kc">False</span><span class="p">])</span><span class="o">.</span><span class="n">repeat</span><span class="p">(</span><span class="n">n_episodes</span><span class="p">)})</span>
<span class="bp">self</span><span class="o">.</span><span class="n">evaluation_batcher</span><span class="o">.</span><span class="n">execute</span><span class="p">(</span><span class="n">n_episodes</span><span class="o">=</span><span class="n">n_episodes</span><span class="p">,</span><span class="n">agent_info</span><span class="o">=</span><span class="n">agent_info</span><span class="p">)</span>
<span class="bp">self</span><span class="o">.</span><span class="n">evaluation_iteration</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">iteration</span>
</pre></div>
</div>
<p>Note that we store the iteration at which the evaluation batcher has been executed</p>
</div>
<div class="section" id="getting-trajectories-without-blocking-the-learning">
<h2>Getting trajectories without blocking the learning<a class="headerlink" href="#getting-trajectories-without-blocking-the-learning" title="Permalink to this headline">¶</a></h2>
<p>Not we can get episodes, but in non blocking mode: the batcher will return <cite>None</cite> if the process of computing episodes is not finished.
If the process is finished, we can 1) compute the reward 2) update the batchers models 3) relaunch the acquisition process. We thus have an evaluation process that runs without blocking the learning, and at maximum speed.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">evaluation_trajectories</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">evaluation_batcher</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">blocking</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
<span class="k">if</span> <span class="ow">not</span> <span class="n">evaluation_trajectories</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span> <span class="c1">#trajectories are available</span>
    <span class="c1">#Compute the cumulated reward</span>
    <span class="n">cumulated_reward</span><span class="o">=</span><span class="p">(</span><span class="n">evaluation_trajectories</span><span class="p">[</span><span class="s2">&quot;_reward&quot;</span><span class="p">]</span><span class="o">*</span><span class="n">evaluation_trajectories</span><span class="o">.</span><span class="n">mask</span><span class="p">())</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">logger</span><span class="o">.</span><span class="n">add_scalar</span><span class="p">(</span><span class="s2">&quot;evaluation_reward&quot;</span><span class="p">,</span><span class="n">cumulated_reward</span><span class="o">.</span><span class="n">item</span><span class="p">(),</span><span class="bp">self</span><span class="o">.</span><span class="n">evaluation_iteration</span><span class="p">)</span>
    <span class="c1">#We reexecute the evaluation batcher (with same value of agent_info and same number of episodes)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">evaluation_batcher</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">learning_model</span><span class="o">.</span><span class="n">state_dict</span><span class="p">())</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">evaluation_iteration</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">iteration</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">evaluation_batcher</span><span class="o">.</span><span class="n">reexecute</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>


           </div>
           
          </div>
          <footer>
  
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
        <a href="a2c.html" class="btn btn-neutral float-right" title="Implemeting Actor-Critic" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right"></span></a>
      
      
        <a href="reinforce.html" class="btn btn-neutral float-left" title="Implementing Reinforce with rlstructures" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left"></span> Previous</a>
      
    </div>
  

  <hr/>

  <div role="contentinfo">
    <p>
        
        &copy; Copyright 2021, Facebook AI Research

    </p>
  </div>
    
    
    
    Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a
    
    <a href="https://github.com/rtfd/sphinx_rtd_theme">theme</a>
    
    provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>

        </div>
      </div>

    </section>

  </div>
  

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>

  
  
    
   

</body>
</html>